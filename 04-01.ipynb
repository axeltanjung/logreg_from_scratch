{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Data\n",
    "# This is an OR logic operation\n",
    "# X1 | X2 | X1 or X2\n",
    "# 0  | 0  | 0\n",
    "# 1  | 0  | 1\n",
    "# 1  | 1  | 1\n",
    "# 0  | 1  | 1\n",
    "data = pd.DataFrame({\"x1\":[0, 1, 1, 0],\n",
    "                     \"x2\":[0, 0, 1, 1],\n",
    "                     \"y\":[0, 1, 1, 1],})\n",
    "X = data[[\"x1\", \"x2\"]]\n",
    "y = data[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate=0.01,\n",
    "        max_iter=100_000,\n",
    "        tol=1e-4\n",
    "    ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Fit the model according to the given training data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            Training vector, where 'n_samples' is the number of samples and\n",
    "            'n_features' is the number of features\n",
    "\n",
    "        y : array-like of shape {n_samples, }\n",
    "            Target vector relative to X \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "            Fitted estimator\n",
    "        '''\n",
    "        # Update the data\n",
    "        X = np.array(X).copy()\n",
    "        y = np.array(y).copy()\n",
    "\n",
    "        # Extract the data & feature size\n",
    "        self.coef_ = np.zeros(self.n_features)\n",
    "        self.intercept_ = 0\n",
    "\n",
    "        # Tune the parameters\n",
    "        for i in range(self.max_iter):\n",
    "            # Make a new prediction\n",
    "            y_pred = self.predict_proba(X)\n",
    "\n",
    "            # Calculate the gradient\n",
    "            grad_coef_ = -(y - y_pred).dot(X) / self.n_samples\n",
    "            grad_intercept_ = -(y - y_pred).dot(np.ones(self.n_samples)) / self.n_samples\n",
    "\n",
    "            # Update parameter\n",
    "            self.coef_ -= self.learning_rate * grad_coef_ \n",
    "            self.intercept_ -= self.learning_rate * grad_intercept_\n",
    "\n",
    "            # Break the itteration\n",
    "            grad_stack_ = np.hstack((grad_coef_, grad_intercept_))\n",
    "            if all(np.abs(grad_stack_) < self.tol):\n",
    "                break\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\"\n",
    "        Probability estimates.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Vector to be scored, where 'n_samples' is the number of samples and\n",
    "            'n_features' is the number of features\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        proba : array-like of shape (n_samples,)\n",
    "            Returns of the probability of the samples for each class in the model,\n",
    "                 \n",
    "        \"\"\"\n",
    "        # Calculate the log odds\n",
    "        logits = np.dot(X, self.coef_) + self.intercept_\n",
    "\n",
    "        # Calculate the probability using sigmoid function\n",
    "        # sigmoid (x) = 1 / ( 1 + e ^(x))\n",
    "        proba = 1. / (1 + np.exp(-logits))\n",
    "\n",
    "        return proba\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'n_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m clf \u001b[39m=\u001b[39m LogisticRegression()\n\u001b[1;32m----> 2\u001b[0m clf\u001b[39m.\u001b[39;49mfit(X,y)\n",
      "Cell \u001b[1;32mIn[5], line 35\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     32\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(y)\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m     34\u001b[0m \u001b[39m# Extract the data & feature size\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_features)\n\u001b[0;32m     36\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_ \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     38\u001b[0m \u001b[39m# Tune the parameters\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'n_features'"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m clf\u001b[39m.\u001b[39;49mpredict_proba(X)\n",
      "Cell \u001b[1;32mIn[5], line 72\u001b[0m, in \u001b[0;36mLogisticRegression.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39m\"\"\"\"\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mProbability estimates.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39m         \u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m# Calculate the log odds\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m logits \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(X, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoef_) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_\n\u001b[0;32m     74\u001b[0m \u001b[39m# Calculate the probability using sigmoid function\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[39m# sigmoid (x) = 1 / ( 1 + e ^(x))\u001b[39;00m\n\u001b[0;32m     76\u001b[0m proba \u001b[39m=\u001b[39m \u001b[39m1.\u001b[39m \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mlogits))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "clf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
